{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do radom search for hyperparameter tuning for a MLP model on the full balanced dataset\n",
    "data = pd.read_csv('C:/Users/Rasmu/OneDrive/DTU/Kunstig intelligens og Data/7. semestre/Bachelor/Machine-Learning-For-Network-Traffic-Analysis/Data/combined_data/final_data_balanced.csv')\n",
    "\n",
    "X = data.drop(['label', 'detailed_label'], axis=1)\n",
    "y = data['label']\n",
    "\n",
    "#check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#Define the parameter grid\n",
    "param_grid = {\n",
    "    'units_layer0': [128, 256, 512],\n",
    "    'units_layer1': [32, 64, 128],\n",
    "    'units_layer2': [8, 16, 32],\n",
    "    'units_layer3': [2, 4, 8],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20, 30, 40, 50],\n",
    "    'dropout_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'learning_rate': loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "n_iter = 60 #Times to try different hyperparameters\n",
    "random_search = list(ParameterSampler(param_grid, n_iter=n_iter, random_state=42)) #Define the 'random search'\n",
    "\n",
    "n_splits = 5 #Number of K-folds\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42) #Create the K-folds\n",
    "\n",
    "#MLP function\n",
    "def create_mlp(parameter, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(input_shape,)))\n",
    "    model.add(Dense(parameter['units_layer0'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(parameter['units_layer1'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(parameter['units_layer2'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(parameter['units_layer3'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=parameter['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Here the code runs the random search\n",
    "results = []\n",
    "for i, parameter in enumerate(random_search):\n",
    "    print(f\"Trial {i+1}/{n_iter}: {parameter}\")\n",
    "    fold_accuracies = []\n",
    "    fold_losses = []\n",
    "    \n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        model = create_mlp(parameter, input_shape=X.shape[1])\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        #Train by using GPU, if available otherwise use CPU\n",
    "        with tf.device('/GPU:0'):\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=parameter['epochs'], batch_size=parameter['batch_size'], callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        fold_accuracies.append(val_accuracy)\n",
    "        fold_losses.append(val_loss)\n",
    "    \n",
    "    #Average results across folds, to ensure that the chosen model is not overfitting\n",
    "    avg_accuracy = sum(fold_accuracies) / n_splits\n",
    "    avg_loss = sum(fold_losses) / n_splits\n",
    "    results.append({'parameters': parameter, 'avg_loss': avg_loss, 'avg_accuracy': avg_accuracy})\n",
    "\n",
    "#Print best configuration\n",
    "best_model = max(results, key=lambda x: x['avg_accuracy'])\n",
    "print(\"Best parameters:\", best_model['params'])\n",
    "print(\"Validation Accuracy:\", best_model['avg_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do radom search for hyperparameter tuning for a MLP model on the selected features\n",
    "data_path = 'C:/Users/Rasmu/OneDrive/DTU/Kunstig intelligens og Data/7. semestre/Bachelor/Machine-Learning-For-Network-Traffic-Analysis/Data/combined_data/final_data_balanced.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "#Only use the selected features\n",
    "X = data[['conn_state_RSTOS0', 'orig_pkts', 'resp_bytes', 'resp_pkts', 'orig_bytes']]\n",
    "y= data['label']\n",
    "\n",
    "#check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#Define the parameter grid\n",
    "param_grid = {\n",
    "    'units_layer0': [128, 256, 512],\n",
    "    'units_layer1': [32, 64, 128],\n",
    "    'units_layer2': [8, 16, 32],\n",
    "    'units_layer3': [4, 8],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20, 30, 40, 50],\n",
    "    'dropout_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'learning_rate': loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "n_iter = 60 #Times to try different hyperparameters\n",
    "random_search = list(ParameterSampler(param_grid, n_iter=n_iter, random_state=42)) #Define teh 'random search'\n",
    "\n",
    "\n",
    "n_splits = 5 #Number of K-folds\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42) #Create the K-folds\n",
    "\n",
    "#MLP function\n",
    "def create_mlp(parameter, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(input_shape,)))\n",
    "    model.add(Dense(parameter['units_layer0'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(parameter['units_layer1'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(parameter['units_layer2'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(parameter['units_layer3'], activation='relu'))\n",
    "    model.add(Dropout(parameter['dropout_rate']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=parameter['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Here the code runs the random search\n",
    "results = []\n",
    "for i, parameter in enumerate(random_search):\n",
    "    print(f\"Trial {i+1}/{n_iter}: {parameter}\")\n",
    "    fold_accuracies = []\n",
    "    fold_losses = []\n",
    "    \n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        model = create_mlp(parameter, input_shape=X.shape[1])\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        #Train by using GPU, if available otherwise use CPU\n",
    "        with tf.device('/GPU:0'):\n",
    "            model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=parameter['epochs'], batch_size=parameter['batch_size'], callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        fold_accuracies.append(val_accuracy)\n",
    "        fold_losses.append(val_loss)\n",
    "    \n",
    "    #Average results across folds, to ensure that the chosen model is not overfitting\n",
    "    avg_accuracy = sum(fold_accuracies) / n_splits\n",
    "    avg_loss = sum(fold_losses) / n_splits\n",
    "    results.append({'parameters': parameter, 'avg_loss': avg_loss, 'avg_accuracy': avg_accuracy})\n",
    "\n",
    "#Print best configuration\n",
    "best_model = max(results, key=lambda x: x['avg_accuracy'])\n",
    "print(\"Best parameters:\", best_model['params'])\n",
    "print(\"Validation Accuracy:\", best_model['avg_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network_traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
