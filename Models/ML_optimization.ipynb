{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data for the to hyperparameter tuning\n",
    "data = pd.read_csv('final_data_balanced.csv')\n",
    "X = data.drop(columns=['label', 'detailed_label'])\n",
    "y = data['label']\n",
    "X_reduced = data[['conn_state_RSTOS0', 'orig_pkts', 'orig_bytes', 'resp_pkts', 'conn_state_SF']]\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization on ML models on the balanced dataset\n",
    "#Decision Tree\n",
    "parameters_dist_tree = {\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = RandomizedSearchCV(tree, parameters_dist_tree, n_iter=60, cv=stratified_cv, random_state=42) #n-iter is the number of iterations to optimize over\n",
    "tree_cv.fit(X, y)\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n",
    "\n",
    "#Random Forest\n",
    "parameter_dist_rf = {\n",
    "    \"n_estimators\": randint(50, 200),\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "forest = RandomForestClassifier()\n",
    "forest_cv = RandomizedSearchCV(forest, parameter_dist_rf, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "forest_cv.fit(X, y)\n",
    "print(\"Tuned Random Forest Parameters: {}\".format(forest_cv.best_params_))\n",
    "print(\"Best score is {}\".format(forest_cv.best_score_))\n",
    "\n",
    "#Gradient Boosting\n",
    "parameter_dist_gb = {\n",
    "    \"n_estimators\": randint(50, 150),\n",
    "    \"learning_rate\": uniform(0.01, 0.1),\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"subsample\": uniform(0.5, 0.5)\n",
    "}\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "grad_boost_cv = RandomizedSearchCV(grad_boost, parameter_dist_gb, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "grad_boost_cv.fit(X, y)\n",
    "print(\"Tuned Gradient Boosting Parameters: {}\".format(grad_boost_cv.best_params_))\n",
    "print(\"Best score is {}\".format(grad_boost_cv.best_score_))\n",
    "\n",
    "#K-Nearest Neighbors\n",
    "parameter_dist_knn = {\n",
    "    \"n_neighbors\": randint(1, 20),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"p\": [1, 2],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = RandomizedSearchCV(knn, parameter_dist_knn, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "knn_cv.fit(X, y)\n",
    "print(\"Tuned K-Nearest Neighbors Parameters: {}\".format(knn_cv.best_params_))\n",
    "print(\"Best score is {}\".format(knn_cv.best_score_))\n",
    "\n",
    "#Histogram Gradient Boosting\n",
    "parameter_dist_hgb = {\n",
    "    \"learning_rate\": uniform(0.01, 0.1),\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_leaf_nodes\": randint(10, 50),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"l2_regularization\": uniform(0.0, 1.0),\n",
    "    \"max_bins\": randint(100, 255)\n",
    "}\n",
    "hist_grad_boost = HistGradientBoostingClassifier()\n",
    "hist_grad_boost_cv = RandomizedSearchCV(hist_grad_boost, parameter_dist_hgb, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "hist_grad_boost_cv.fit(X, y)\n",
    "print(\"Tuned HistGradientBoostingClassifier Parameters: {}\".format(hist_grad_boost_cv.best_params_))\n",
    "print(\"Best score is {}\".format(hist_grad_boost_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization on ML models on the feature selected balanced dataset\n",
    "#Decision Tree\n",
    "parameters_dist_tree = {\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = RandomizedSearchCV(tree, parameters_dist_tree, n_iter=60, cv=stratified_cv, random_state=42) #n-iter is the number of iterations to optimize over\n",
    "tree_cv.fit(X, y)\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n",
    "\n",
    "#Random Forest\n",
    "parameter_dist_rf = {\n",
    "    \"n_estimators\": randint(50, 200),\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "forest = RandomForestClassifier()\n",
    "forest_cv = RandomizedSearchCV(forest, parameter_dist_rf, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "forest_cv.fit(X, y)\n",
    "print(\"Tuned Random Forest Parameters: {}\".format(forest_cv.best_params_))\n",
    "print(\"Best score is {}\".format(forest_cv.best_score_))\n",
    "\n",
    "#Gradient Boosting\n",
    "parameter_dist_gb = {\n",
    "    \"n_estimators\": randint(50, 150),\n",
    "    \"learning_rate\": uniform(0.01, 0.1),\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"subsample\": uniform(0.5, 0.5)\n",
    "}\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "grad_boost_cv = RandomizedSearchCV(grad_boost, parameter_dist_gb, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "grad_boost_cv.fit(X, y)\n",
    "print(\"Tuned Gradient Boosting Parameters: {}\".format(grad_boost_cv.best_params_))\n",
    "print(\"Best score is {}\".format(grad_boost_cv.best_score_))\n",
    "\n",
    "#K-Nearest Neighbors\n",
    "parameter_dist_knn = {\n",
    "    \"n_neighbors\": randint(1, 20),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"p\": [1, 2],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = RandomizedSearchCV(knn, parameter_dist_knn, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "knn_cv.fit(X, y)\n",
    "print(\"Tuned K-Nearest Neighbors Parameters: {}\".format(knn_cv.best_params_))\n",
    "print(\"Best score is {}\".format(knn_cv.best_score_))\n",
    "\n",
    "#Histogram Gradient Boosting\n",
    "parameter_dist_hgb = {\n",
    "    \"learning_rate\": uniform(0.01, 0.1),\n",
    "    \"max_depth\": [3, 5, 10, 15, 20, None],\n",
    "    \"max_leaf_nodes\": randint(10, 50),\n",
    "    \"min_samples_leaf\": randint(1, 9),\n",
    "    \"l2_regularization\": uniform(0.0, 1.0),\n",
    "    \"max_bins\": randint(100, 255)\n",
    "}\n",
    "hist_grad_boost = HistGradientBoostingClassifier()\n",
    "hist_grad_boost_cv = RandomizedSearchCV(hist_grad_boost, parameter_dist_hgb, n_iter=60, cv=stratified_cv, random_state=42)\n",
    "hist_grad_boost_cv.fit(X, y)\n",
    "print(\"Tuned HistGradientBoostingClassifier Parameters: {}\".format(hist_grad_boost_cv.best_params_))\n",
    "print(\"Best score is {}\".format(hist_grad_boost_cv.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
