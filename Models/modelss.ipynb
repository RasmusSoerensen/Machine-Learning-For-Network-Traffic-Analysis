{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (accuracy_score, f1_score, recall_score, precision_score,  roc_curve, auc, confusion_matrix, matthews_corrcoef)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Input, Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from numpy import interp\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and define the data\n",
    "balanced_data = pd.read_csv('C:/Users/Rasmu/OneDrive/DTU\\Kunstig intelligens og Data/7. semestre/Bachelor/Machine-Learning-For-Network-Traffic-Analysis/Data/combined_data/final_data_balanced.csv')\n",
    "imbalanced_data = pd.read_csv('C:/Users/Rasmu/OneDrive/DTU\\Kunstig intelligens og Data/7. semestre/Bachelor/Machine-Learning-For-Network-Traffic-Analysis/Data/combined_data/final_data_imbalanced.csv')\n",
    "#The 4 datasets\n",
    "X = balanced_data.drop(columns=['label', 'detailed_label'])\n",
    "y = balanced_data['label']\n",
    "X_imbalanced = imbalanced_data.drop(columns=['label', 'detailed_label'])\n",
    "y_imbalanced = imbalanced_data['label']\n",
    "X_selected_ba = balanced_data[['conn_state_RSTOS0', 'orig_pkts', 'resp_bytes', 'resp_pkts', 'orig_bytes']]\n",
    "y_selected_ba = balanced_data['label']\n",
    "X_selected_im = imbalanced_data[['conn_state_RSTOS0', 'orig_pkts', 'resp_bytes', 'resp_pkts', 'orig_bytes']]\n",
    "y_selected_im = imbalanced_data['label']\n",
    "\n",
    "#Define the models based on the best hyperparameters from the hyperoptimization.ipynb file\n",
    "ml_models = [\n",
    "    DecisionTreeClassifier(criterion='entropy', max_depth=15, max_features='sqrt', min_samples_leaf=1, min_samples_split=5),\n",
    "    RandomForestClassifier(criterion='gini', max_depth=15, max_features='log2', min_samples_leaf=2, min_samples_split=3, n_estimators=58),\n",
    "    GaussianNB(),\n",
    "    GradientBoostingClassifier(learning_rate=0.036847485689015684, max_depth=10, max_features='log2', min_samples_leaf=1, min_samples_split=3, n_estimators=144, subsample=0.9221065703631557),\n",
    "    KNeighborsClassifier(algorithm='auto', n_neighbors=4, p=2, weights='uniform'),\n",
    "    HistGradientBoostingClassifier(l2_regularization=0.7111495324380178, learning_rate=0.09095010461397154, max_bins=247, max_depth=15, max_leaf_nodes=42, min_samples_leaf=8),\n",
    "]\n",
    "\n",
    "models_names = ['Decision Tree', 'Gradient Boosting', 'KNN', 'Naive Bayes', 'Random Forest', 'Hist Gradient Boosting','MLP']\n",
    "\n",
    "#Define the result lists\n",
    "results = []\n",
    "std = []\n",
    "average_roc_auc = []\n",
    "average_std_roc_auc = []\n",
    "accuracy_scores = []\n",
    "accuracy_std = []\n",
    "f1_scores = []\n",
    "f1_std = []\n",
    "precision_scores = []\n",
    "precision_std = []\n",
    "recall_scores = []\n",
    "recall_std = []\n",
    "MCC_values = []\n",
    "MCC_std = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the models on the balanced dataset\n",
    "\n",
    "#Ensure that the folds are stratified\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "#Plotting helper function\n",
    "def plot_confusion_matrix_with_std(avg_cm, std_cm, model_name):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Mean Confusion Matrix for {model_name}')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(std_cm, annot=True, fmt='.2f', cmap='Oranges', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'STD of Mean Confusion Matrix for {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Models to store model specific data\n",
    "model_mean_fprs = {} \n",
    "model_mean_tprs = {}\n",
    "model_std_tprs = {}\n",
    "model_auc = {}\n",
    "\n",
    "fold = 1 #initialize fold counter\n",
    "\n",
    "for model in ml_models:\n",
    "    tpr_list = []\n",
    "    roc_auc_list = []\n",
    "    cm_list = [] \n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        #Calculate the different metrics\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_list.append(cm)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "        tpr_interpolated = np.interp(mean_fpr, fpr, tpr)\n",
    "        tpr_interpolated[0] = 0.0\n",
    "        tpr_list.append(tpr_interpolated)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        MCC_values.append(mcc)\n",
    "\n",
    "        results.append([accuracy, f1, precision, recall, mcc])\n",
    "        std.append([np.std(accuracy_scores), np.std(f1_scores), np.std(precision_scores), np.std(recall_scores), np.std(MCC_values)])\n",
    "\n",
    "        #Save the model\n",
    "        model_filename = f\"{model.__class__.__name__}_fold_{fold}.joblib\"\n",
    "        joblib.dump(model, model_filename)\n",
    "\n",
    "        fold += 1\n",
    "    fold = 1\n",
    "\n",
    "    #Calculating and showing the mean and standard deviation confusion matrix \n",
    "    mean_tpr = np.mean(tpr_list, axis=0)\n",
    "    std_tpr = np.std(tpr_list, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    model_mean_fprs[model.__class__.__name__] = mean_fpr\n",
    "    model_mean_tprs[model.__class__.__name__] = mean_tpr\n",
    "    model_std_tprs[model.__class__.__name__] = std_tpr\n",
    "    model_auc[model.__class__.__name__] = (np.mean(roc_auc_list), np.std(roc_auc_list))\n",
    "    avg_cm = np.mean(cm_list, axis=0)\n",
    "    std_cm = np.std(cm_list, axis=0)\n",
    "    plot_confusion_matrix_with_std(avg_cm, std_cm, model.__class__.__name__)\n",
    "\n",
    "#List for the MLP model\n",
    "mlp_tpr_list = []\n",
    "mlp_roc_auc_list = []\n",
    "mlp_cm_list = []\n",
    "fold_accuracies = []\n",
    "history_data = []\n",
    "\n",
    "#MLP model with hyperparameters from hyperoptimization.ipynb\n",
    "for train_index, val_index in kf.split(X,y):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    input_dim = X_train.shape[1]\n",
    "    mlp_model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.05),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.05),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.05),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.05),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0007707278591463597)\n",
    "    mlp_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = mlp_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val),\n",
    "                  callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    history_data.append(history.history)\n",
    "\n",
    "    y_pred = (mlp_model.predict(X_val) > 0.5).astype(int).ravel()\n",
    "    y_prob = mlp_model.predict(X_val).ravel()\n",
    "    mlp_model.summary()\n",
    "\n",
    "    #Calculate the different metrics\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    mlp_cm_list.append(cm)\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    mlp_roc_auc_list.append(roc_auc)\n",
    "    tpr_interpolated = np.interp(mean_fpr, fpr, tpr)\n",
    "    tpr_interpolated[0] = 0.0\n",
    "    mlp_tpr_list.append(tpr_interpolated)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    MCC_values.append(mcc)\n",
    "    results.append([accuracy, f1, precision, recall, mcc])\n",
    "    std.append([np.std(accuracy_scores), np.std(f1_scores), np.std(precision_scores), np.std(recall_scores), np.std(MCC_values)])\n",
    "\n",
    "    #Save the models\n",
    "    model_filename = f\"MLP_fold_{fold}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    fold += 1\n",
    "\n",
    "#Confusion matrix for MLP\n",
    "avg_mlp_cm = np.mean(mlp_cm_list, axis=0)\n",
    "std_mlp_cm = np.std(mlp_cm_list, axis=0)\n",
    "plot_confusion_matrix_with_std(avg_mlp_cm, std_mlp_cm, \"MLP\")\n",
    "\n",
    "mean_mlp_tpr = np.mean(mlp_tpr_list, axis=0)\n",
    "std_mlp_tpr = np.std(mlp_tpr_list, axis=0)\n",
    "mean_mlp_tpr[-1] = 1.0\n",
    "mean_mlp_auc = np.mean(mlp_roc_auc_list)\n",
    "std_mlp_auc = np.std(mlp_roc_auc_list)\n",
    "\n",
    "#ROC curve for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model_name in model_mean_fprs.keys():\n",
    "    plt.plot(model_mean_fprs[model_name], model_mean_tprs[model_name], label=f\"{model_name} (AUC = {model_auc[model_name][0]:.4f} ± {model_auc[model_name][1]:.4f})\")\n",
    "    plt.fill_between(model_mean_fprs[model_name], model_mean_tprs[model_name] - model_std_tprs[model_name], model_mean_tprs[model_name] + model_std_tprs[model_name], alpha=0.2)\n",
    "plt.plot(mean_fpr, mean_mlp_tpr, label=f\"MLP (AUC = {mean_mlp_auc:.4f} ± {std_mlp_auc:.4f})\", color='blue')\n",
    "plt.fill_between(mean_fpr, mean_mlp_tpr - std_mlp_tpr, mean_mlp_tpr + std_mlp_tpr, color='blue', alpha=0.2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation for Each Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Create the table with the results\n",
    "models_names = ['Decision Tree', 'Random Forest', 'Naive Bayes', 'Gradient Boosting', 'KNN', 'Hist Gradient Boosting', 'MLP']\n",
    "DT_results = results[0:5]\n",
    "RF_results = results[5:10]\n",
    "GNB_results = results[10:15]\n",
    "GB_results = results[15:20]\n",
    "KNN_results = results[20:25]\n",
    "HGB_results = results[25:30]\n",
    "MLP_results = results[30:35]\n",
    "DT_std = std[0]\n",
    "RF_std = std[1]\n",
    "GNB_std = std[2]\n",
    "GB_std = std[3]\n",
    "KNN_std = std[4]\n",
    "HGB_std = std[5]\n",
    "MLP_std = std[6]\n",
    "result_data = [\n",
    "    [np.mean(DT_results[0]), np.mean(DT_results[1]), np.mean(DT_results[2]), np.mean(DT_results[3]), np.mean(DT_results[4])],\n",
    "    [np.mean(RF_results[0]), np.mean(RF_results[1]), np.mean(RF_results[2]), np.mean(RF_results[3]), np.mean(RF_results[4])],\n",
    "    [np.mean(GNB_results[0]), np.mean(GNB_results[1]), np.mean(GNB_results[2]), np.mean(GNB_results[3]), np.mean(GNB_results[4])],\n",
    "    [np.mean(GB_results[0]), np.mean(GB_results[1]), np.mean(GB_results[2]), np.mean(GB_results[3]), np.mean(GB_results[4])],\n",
    "    [np.mean(KNN_results[0]), np.mean(KNN_results[1]), np.mean(KNN_results[2]), np.mean(KNN_results[3]), np.mean(KNN_results[4])],\n",
    "    [np.mean(HGB_results[0]), np.mean(HGB_results[1]), np.mean(HGB_results[2]), np.mean(HGB_results[3]), np.mean(HGB_results[4])],\n",
    "    [np.mean(MLP_results[0]), np.mean(MLP_results[1]), np.mean(MLP_results[2]), np.mean(MLP_results[3]), np.mean(MLP_results[4])]\n",
    "]\n",
    "\n",
    "results_std = [\n",
    "    [np.mean(DT_std[0]), np.mean(DT_std[1]), np.mean(DT_std[2]), np.mean(DT_std[3]), np.mean(DT_std[4])],\n",
    "    [np.mean(GB_std[0]), np.mean(GB_std[1]), np.mean(GB_std[2]), np.mean(GB_std[3]), np.mean(GB_std[4])],\n",
    "    [np.mean(KNN_std[0]), np.mean(KNN_std[1]), np.mean(KNN_std[2]), np.mean(KNN_std[3]), np.mean(KNN_std[4])],\n",
    "    [np.mean(GNB_std[0]), np.mean(GNB_std[1]), np.mean(GNB_std[2]), np.mean(GNB_std[3]), np.mean(GNB_std[4])],\n",
    "    [np.mean(RF_std[0]), np.mean(RF_std[1]), np.mean(RF_std[2]), np.mean(RF_std[3]), np.mean(RF_std[4])],\n",
    "    [np.mean(HGB_std[0]), np.mean(HGB_std[1]), np.mean(HGB_std[2]), np.mean(HGB_std[3]), np.mean(HGB_std[4])],\n",
    "    [np.mean(MLP_std[0]), np.mean(MLP_std[1]), np.mean(MLP_std[2]), np.mean(MLP_std[3]), np.mean(MLP_std[4])]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "result_df_std = pd.DataFrame(results_std, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "combined_df = result_df.apply(lambda row: [f\"{mean:.4f} ± {std:.4f}\" for mean, std in zip(row, result_df_std.loc[row.name])], axis=1)\n",
    "combined_df = pd.DataFrame(combined_df.tolist(), columns=result_df.columns, index=result_df.index)\n",
    "print(combined_df)\n",
    "\n",
    "#MLP model accuracy and loss plot\n",
    "#Initialize lists to store loss and accuracy for each fold\n",
    "loss_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "accuracy_per_fold = []\n",
    "val_accuracy_per_fold = []\n",
    "\n",
    "#Extract the loss and accuracy for each fold\n",
    "for history in history_data:\n",
    "    loss_per_fold.append(history['loss'])\n",
    "    val_loss_per_fold.append(history['val_loss'])\n",
    "    accuracy_per_fold.append(history['accuracy'])\n",
    "    val_accuracy_per_fold.append(history['val_accuracy'])\n",
    "\n",
    "#Calculate the values for the accuracy and loss plots\n",
    "def pad_list(data, max_len):\n",
    "    return data + [np.nan] * (max_len - len(data))\n",
    "max_len = max(len(history['loss']) for history in history_data)\n",
    "loss_per_fold = [pad_list(history['loss'], max_len) for history in history_data]\n",
    "val_loss_per_fold = [pad_list(history['val_loss'], max_len) for history in history_data]\n",
    "accuracy_per_fold = [pad_list(history['accuracy'], max_len) for history in history_data]\n",
    "val_accuracy_per_fold = [pad_list(history['val_accuracy'], max_len) for history in history_data]\n",
    "mean_loss = np.nanmean(loss_per_fold, axis=0)\n",
    "std_loss = np.nanstd(loss_per_fold, axis=0)\n",
    "mean_val_loss = np.nanmean(val_loss_per_fold, axis=0)\n",
    "std_val_loss = np.nanstd(val_loss_per_fold, axis=0)\n",
    "mean_accuracy = np.nanmean(accuracy_per_fold, axis=0)\n",
    "std_accuracy = np.nanstd(accuracy_per_fold, axis=0)\n",
    "mean_val_accuracy = np.nanmean(val_accuracy_per_fold, axis=0)\n",
    "std_val_accuracy = np.nanstd(val_accuracy_per_fold, axis=0)\n",
    "\n",
    "#Plot Loss\n",
    "epochs = range(1, len(mean_loss) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, mean_loss, label='Training Loss', color='blue')\n",
    "plt.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, color='blue', alpha=0.2)\n",
    "plt.plot(epochs, mean_val_loss, label='Validation Loss', color='orange')\n",
    "plt.fill_between(epochs, mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, color='orange', alpha=0.2)\n",
    "plt.title('Training and Validation Loss (Mean ± Std)', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "#Plot Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, mean_accuracy, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(epochs, mean_accuracy - std_accuracy, mean_accuracy + std_accuracy, color='blue', alpha=0.2)\n",
    "plt.plot(epochs, mean_val_accuracy, label='Validation Accuracy', color='orange')\n",
    "plt.fill_between(epochs, mean_val_accuracy - std_val_accuracy, mean_val_accuracy + std_val_accuracy, color='orange', alpha=0.2)\n",
    "plt.title('Training and Validation Accuracy (Mean ± Std)', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the models on the imbalanced dataset\n",
    "#Define the result lists\n",
    "results = []\n",
    "std = []\n",
    "average_roc_auc = []\n",
    "average_std_roc_auc = []\n",
    "accuracy_scores = []\n",
    "accuracy_std = []\n",
    "f1_scores = []\n",
    "f1_std = []\n",
    "precision_scores = []\n",
    "precision_std = []\n",
    "recall_scores = []\n",
    "recall_std = []\n",
    "MCC_values = []\n",
    "MCC_std = []\n",
    "\n",
    "#Define data\n",
    "x_test = X_imbalanced\n",
    "y_test = y_imbalanced\n",
    "\n",
    "\n",
    "#ROC curve plotting settings\n",
    "fpr_mean = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "#Model types and folds\n",
    "model_types = ['DecisionTreeClassifier', 'RandomForestClassifier', 'GaussianNB', 'GradientBoostingClassifier', 'KNeighborsClassifier', 'HistGradientBoostingClassifier', 'MLP']\n",
    "folds_per_model = 5\n",
    "models = []\n",
    "std = [[] for _ in range(len(model_types) * folds_per_model)]\n",
    "conf_matrices = {model_type: [] for model_type in model_types}\n",
    "\n",
    "#Load models from files\n",
    "models = []\n",
    "for model_type in model_types:\n",
    "    for fold in range(folds_per_model):\n",
    "        model_filename = f\"{model_type.replace(' ', '')}_fold_{fold + 1}.joblib\"\n",
    "        try:\n",
    "            loaded_model = joblib.load(model_filename)\n",
    "            models.append(loaded_model)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Model file not found: {model_filename}\")\n",
    "            models.append(None)\n",
    "\n",
    "#Test each model\n",
    "for i, model_type in enumerate(model_types):\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    conf_matrix_sum = np.zeros((2, 2))\n",
    "\n",
    "    for fold in range(folds_per_model):\n",
    "        model = models[i * folds_per_model + fold]\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_prob = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "        #Calculate the different metrics\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        tprs.append(np.interp(fpr_mean, fpr, tpr))\n",
    "        aucs.append(roc_auc)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        MCC_values.append(mcc)\n",
    "        results.append([accuracy, f1, precision, recall, mcc])\n",
    "        current_model_index = i * folds_per_model + fold\n",
    "        std[current_model_index].extend([np.std(y_pred), np.std(f1), np.std(precision), np.std(recall), np.std(mcc)])\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        conf_matrices[model_type].append(conf_matrix)\n",
    "        conf_matrix_sum += conf_matrix\n",
    "\n",
    "    # Calculate values for ROC curve\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(fpr_mean, mean_tpr, label=f'{model_type} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    plt.fill_between(fpr_mean, mean_tpr - std_tpr, mean_tpr + std_tpr, alpha=0.2)\n",
    "\n",
    "#ROC plot\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for All Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Plot Mean and Standard Deviation Confusion Matrices\n",
    "for model_type, conf_matrices_list in conf_matrices.items():\n",
    "    conf_matrices_array = np.array(conf_matrices_list)\n",
    "    mean_conf_matrix = np.mean(conf_matrices_array, axis=0)\n",
    "    std_conf_matrix = np.std(conf_matrices_array, axis=0)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    sns.heatmap(mean_conf_matrix, annot=True, fmt='.2f', cmap='Blues', xticklabels=['Benign', 'Malicious'], yticklabels=['Benign', 'Malicious'], ax=axes[0])\n",
    "    axes[0].set_title(f'Mean Confusion Matrix for {model_type}')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    sns.heatmap(std_conf_matrix, annot=True, fmt='.2f', cmap='Oranges', xticklabels=['Benign', 'Malicious'], yticklabels=['Benign', 'Malicious'], ax=axes[1])\n",
    "    axes[1].set_title(f'Standard Deviation Confusion Matrix for {model_type}')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Create the table with the results\n",
    "models_names = ['Decision Tree', 'Random Forest', 'Naive Bayes', 'Gradient Boosting', 'KNN', 'Hist Gradient Boosting', 'MLP']\n",
    "DT_results = results[0:5]\n",
    "RF_results = results[5:10]\n",
    "GNB_results = results[10:15]\n",
    "GB_results = results[15:20]\n",
    "KNN_results = results[20:25]\n",
    "HGB_results = results[25:30]\n",
    "MLP_results = results[30:35]\n",
    "DT_std = std[0]\n",
    "RF_std = std[1]\n",
    "GNB_std = std[2]\n",
    "GB_std = std[3]\n",
    "KNN_std = std[4]\n",
    "HGB_std = std[5]\n",
    "MLP_std = std[6]\n",
    "result_data = [\n",
    "    [np.mean(DT_results[0]), np.mean(DT_results[1]), np.mean(DT_results[2]), np.mean(DT_results[3]), np.mean(DT_results[4])],\n",
    "    [np.mean(RF_results[0]), np.mean(RF_results[1]), np.mean(RF_results[2]), np.mean(RF_results[3]), np.mean(RF_results[4])],\n",
    "    [np.mean(GNB_results[0]), np.mean(GNB_results[1]), np.mean(GNB_results[2]), np.mean(GNB_results[3]), np.mean(GNB_results[4])],\n",
    "    [np.mean(GB_results[0]), np.mean(GB_results[1]), np.mean(GB_results[2]), np.mean(GB_results[3]), np.mean(GB_results[4])],\n",
    "    [np.mean(KNN_results[0]), np.mean(KNN_results[1]), np.mean(KNN_results[2]), np.mean(KNN_results[3]), np.mean(KNN_results[4])],\n",
    "    [np.mean(HGB_results[0]), np.mean(HGB_results[1]), np.mean(HGB_results[2]), np.mean(HGB_results[3]), np.mean(HGB_results[4])],\n",
    "    [np.mean(MLP_results[0]), np.mean(MLP_results[1]), np.mean(MLP_results[2]), np.mean(MLP_results[3]), np.mean(MLP_results[4])]\n",
    "]\n",
    "\n",
    "results_std = [\n",
    "    [np.mean(DT_std[0]), np.mean(DT_std[1]), np.mean(DT_std[2]), np.mean(DT_std[3]), np.mean(DT_std[4])],\n",
    "    [np.mean(GB_std[0]), np.mean(GB_std[1]), np.mean(GB_std[2]), np.mean(GB_std[3]), np.mean(GB_std[4])],\n",
    "    [np.mean(KNN_std[0]), np.mean(KNN_std[1]), np.mean(KNN_std[2]), np.mean(KNN_std[3]), np.mean(KNN_std[4])],\n",
    "    [np.mean(GNB_std[0]), np.mean(GNB_std[1]), np.mean(GNB_std[2]), np.mean(GNB_std[3]), np.mean(GNB_std[4])],\n",
    "    [np.mean(RF_std[0]), np.mean(RF_std[1]), np.mean(RF_std[2]), np.mean(RF_std[3]), np.mean(RF_std[4])],\n",
    "    [np.mean(HGB_std[0]), np.mean(HGB_std[1]), np.mean(HGB_std[2]), np.mean(HGB_std[3]), np.mean(HGB_std[4])],\n",
    "    [np.mean(MLP_std[0]), np.mean(MLP_std[1]), np.mean(MLP_std[2]), np.mean(MLP_std[3]), np.mean(MLP_std[4])]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "result_df_std = pd.DataFrame(results_std, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "combined_df = result_df.apply(lambda row: [f\"{mean:.4f} ± {std:.4f}\" for mean, std in zip(row, result_df_std.loc[row.name])], axis=1)\n",
    "combined_df = pd.DataFrame(combined_df.tolist(), columns=result_df.columns, index=result_df.index)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the models on the selected features from the balanced dataset\n",
    "#Define the result lists\n",
    "results = []\n",
    "std = []\n",
    "average_roc_auc = []\n",
    "average_std_roc_auc = []\n",
    "accuracy_scores = []\n",
    "accuracy_std = []\n",
    "f1_scores = []\n",
    "f1_std = []\n",
    "precision_scores = []\n",
    "precision_std = []\n",
    "recall_scores = []\n",
    "recall_std = []\n",
    "MCC_values = []\n",
    "MCC_std = []\n",
    "\n",
    "#Define the models based on the best hyperparameters from the hyperoptimization.ipynb file\n",
    "ml_models = [\n",
    "    DecisionTreeClassifier(criterion='gini', max_depth=None, max_features='sqrt', min_samples_leaf=1, min_samples_split=2),\n",
    "    RandomForestClassifier(criterion='gini', max_depth=15, max_features='sqrt', min_samples_leaf=1, min_samples_split=9, n_estimators=177),\n",
    "    GaussianNB(),\n",
    "    GradientBoostingClassifier(learning_rate=0.10356349942209475, max_depth=15, max_features='log2', min_samples_leaf=8, min_samples_split=5, n_estimators=148, subsample=0.6861413832808716),\n",
    "    KNeighborsClassifier(algorithm='kd_tree', n_neighbors=19, p=1, weights='distance'),\n",
    "    HistGradientBoostingClassifier(l2_regularization=0.7111495324380178, learning_rate=0.09095010461397154, max_bins=247, max_depth=15, max_leaf_nodes=42, min_samples_leaf=8),\n",
    "]\n",
    "\n",
    "#Define data\n",
    "X = X_selected_ba\n",
    "y = y_selected_ba\n",
    "\n",
    "#Ensure that the folds are stratified\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  \n",
    "mean_fpr = np.linspace(0, 1, 100) \n",
    "\n",
    "#Plotting helper function\n",
    "def plot_confusion_matrix_with_std(avg_cm, std_cm, model_name):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Mean Confusion Matrix for {model_name}')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(std_cm, annot=True, fmt='.2f', cmap='Oranges', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'STD of Mean Confusion Matrix for {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Models to store model specific data\n",
    "model_mean_fprs = {} \n",
    "model_mean_tprs = {}\n",
    "model_std_tprs = {}\n",
    "model_auc = {}\n",
    "\n",
    "fold = 1 #initialize fold counter\n",
    "\n",
    "for model in ml_models:\n",
    "    tpr_list = []\n",
    "    roc_auc_list = []\n",
    "    cm_list = [] \n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        #Calculate the different metrics\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_list.append(cm)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "        tpr_interpolated = np.interp(mean_fpr, fpr, tpr)\n",
    "        tpr_interpolated[0] = 0.0\n",
    "        tpr_list.append(tpr_interpolated)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        MCC_values.append(mcc)\n",
    "\n",
    "        results.append([accuracy, f1, precision, recall, mcc])\n",
    "        std.append([np.std(accuracy_scores), np.std(f1_scores), np.std(precision_scores), np.std(recall_scores), np.std(MCC_values)])\n",
    "\n",
    "        #Save the model\n",
    "        model_filename = f\"{model.__class__.__name__}_fold_{fold}.joblib\"\n",
    "        joblib.dump(model, model_filename)\n",
    "\n",
    "        fold += 1\n",
    "    fold = 1\n",
    "\n",
    "    #Calculating and showing the mean and standard deviation confusion matrix \n",
    "    mean_tpr = np.mean(tpr_list, axis=0)\n",
    "    std_tpr = np.std(tpr_list, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    model_mean_fprs[model.__class__.__name__] = mean_fpr\n",
    "    model_mean_tprs[model.__class__.__name__] = mean_tpr\n",
    "    model_std_tprs[model.__class__.__name__] = std_tpr\n",
    "    model_auc[model.__class__.__name__] = (np.mean(roc_auc_list), np.std(roc_auc_list))\n",
    "    avg_cm = np.mean(cm_list, axis=0)\n",
    "    std_cm = np.std(cm_list, axis=0)\n",
    "    plot_confusion_matrix_with_std(avg_cm, std_cm, model.__class__.__name__)\n",
    "\n",
    "\n",
    "#List for the MLP model\n",
    "mlp_tpr_list = []\n",
    "mlp_roc_auc_list = []\n",
    "mlp_cm_list = []\n",
    "fold_accuracies = []\n",
    "history_data = []\n",
    "\n",
    "#MLP model with hyperparameters from hyperoptimization.ipynb\n",
    "for train_index, val_index in kf.split(X,y):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    input_dim = X_train.shape[1]\n",
    "    mlp_2_model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.01),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.01),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.01),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.01),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0006915151366011638)\n",
    "    mlp_2_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = mlp_2_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val),\n",
    "                  callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    history_data.append(history.history)\n",
    "    y_pred = (mlp_2_model.predict(X_val) > 0.5).astype(int).ravel()\n",
    "    y_prob = mlp_2_model.predict(X_val).ravel()\n",
    "    mlp_2_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    mlp_cm_list.append(cm)\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    mlp_roc_auc_list.append(roc_auc)\n",
    "    tpr_interpolated = np.interp(mean_fpr, fpr, tpr)\n",
    "    tpr_interpolated[0] = 0.0\n",
    "    mlp_tpr_list.append(tpr_interpolated)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    MCC_values.append(mcc)\n",
    "    results.append([accuracy, f1, precision, recall, mcc])\n",
    "    std.append([np.std(accuracy_scores), np.std(f1_scores), np.std(precision_scores), np.std(recall_scores), np.std(MCC_values)])\n",
    "    model_filename = f\"MLP_fold_{fold}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    fold += 1\n",
    "\n",
    "#Confusion matrix for MLP\n",
    "avg_mlp_cm = np.mean(mlp_cm_list, axis=0)\n",
    "std_mlp_cm = np.std(mlp_cm_list, axis=0)\n",
    "plot_confusion_matrix_with_std(avg_mlp_cm, std_mlp_cm, \"MLP\")\n",
    "\n",
    "mean_mlp_tpr = np.mean(mlp_tpr_list, axis=0)\n",
    "std_mlp_tpr = np.std(mlp_tpr_list, axis=0)\n",
    "mean_mlp_tpr[-1] = 1.0\n",
    "mean_mlp_auc = np.mean(mlp_roc_auc_list)\n",
    "std_mlp_auc = np.std(mlp_roc_auc_list)\n",
    "\n",
    "#ROC curve for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model_name in model_mean_fprs.keys():\n",
    "    plt.plot(model_mean_fprs[model_name], model_mean_tprs[model_name], label=f\"{model_name} (AUC = {model_auc[model_name][0]:.4f} ± {model_auc[model_name][1]:.4f})\")\n",
    "    plt.fill_between(model_mean_fprs[model_name], model_mean_tprs[model_name] - model_std_tprs[model_name], model_mean_tprs[model_name] + model_std_tprs[model_name], alpha=0.2)\n",
    "plt.plot(mean_fpr, mean_mlp_tpr, label=f\"MLP (AUC = {mean_mlp_auc:.4f} ± {std_mlp_auc:.4f})\", color='blue')\n",
    "plt.fill_between(mean_fpr, mean_mlp_tpr - std_mlp_tpr, mean_mlp_tpr + std_mlp_tpr, color='blue', alpha=0.2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve with Standard Deviation for Each Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Create the table with the results\n",
    "models_names = ['Decision Tree', 'Random Forest', 'Naive Bayes', 'Gradient Boosting', 'KNN', 'Hist Gradient Boosting', 'MLP']\n",
    "DT_results = results[0:5]\n",
    "RF_results = results[5:10]\n",
    "GNB_results = results[10:15]\n",
    "GB_results = results[15:20]\n",
    "KNN_results = results[20:25]\n",
    "HGB_results = results[25:30]\n",
    "MLP_results = results[30:35]\n",
    "DT_std = std[0]\n",
    "RF_std = std[1]\n",
    "GNB_std = std[2]\n",
    "GB_std = std[3]\n",
    "KNN_std = std[4]\n",
    "HGB_std = std[5]\n",
    "MLP_std = std[6]\n",
    "result_data = [\n",
    "    [np.mean(DT_results[0]), np.mean(DT_results[1]), np.mean(DT_results[2]), np.mean(DT_results[3]), np.mean(DT_results[4])],\n",
    "    [np.mean(RF_results[0]), np.mean(RF_results[1]), np.mean(RF_results[2]), np.mean(RF_results[3]), np.mean(RF_results[4])],\n",
    "    [np.mean(GNB_results[0]), np.mean(GNB_results[1]), np.mean(GNB_results[2]), np.mean(GNB_results[3]), np.mean(GNB_results[4])],\n",
    "    [np.mean(GB_results[0]), np.mean(GB_results[1]), np.mean(GB_results[2]), np.mean(GB_results[3]), np.mean(GB_results[4])],\n",
    "    [np.mean(KNN_results[0]), np.mean(KNN_results[1]), np.mean(KNN_results[2]), np.mean(KNN_results[3]), np.mean(KNN_results[4])],\n",
    "    [np.mean(HGB_results[0]), np.mean(HGB_results[1]), np.mean(HGB_results[2]), np.mean(HGB_results[3]), np.mean(HGB_results[4])],\n",
    "    [np.mean(MLP_results[0]), np.mean(MLP_results[1]), np.mean(MLP_results[2]), np.mean(MLP_results[3]), np.mean(MLP_results[4])]\n",
    "]\n",
    "\n",
    "results_std = [\n",
    "    [np.mean(DT_std[0]), np.mean(DT_std[1]), np.mean(DT_std[2]), np.mean(DT_std[3]), np.mean(DT_std[4])],\n",
    "    [np.mean(GB_std[0]), np.mean(GB_std[1]), np.mean(GB_std[2]), np.mean(GB_std[3]), np.mean(GB_std[4])],\n",
    "    [np.mean(KNN_std[0]), np.mean(KNN_std[1]), np.mean(KNN_std[2]), np.mean(KNN_std[3]), np.mean(KNN_std[4])],\n",
    "    [np.mean(GNB_std[0]), np.mean(GNB_std[1]), np.mean(GNB_std[2]), np.mean(GNB_std[3]), np.mean(GNB_std[4])],\n",
    "    [np.mean(RF_std[0]), np.mean(RF_std[1]), np.mean(RF_std[2]), np.mean(RF_std[3]), np.mean(RF_std[4])],\n",
    "    [np.mean(HGB_std[0]), np.mean(HGB_std[1]), np.mean(HGB_std[2]), np.mean(HGB_std[3]), np.mean(HGB_std[4])],\n",
    "    [np.mean(MLP_std[0]), np.mean(MLP_std[1]), np.mean(MLP_std[2]), np.mean(MLP_std[3]), np.mean(MLP_std[4])]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "result_df_std = pd.DataFrame(results_std, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "combined_df = result_df.apply(lambda row: [f\"{mean:.4f} ± {std:.4f}\" for mean, std in zip(row, result_df_std.loc[row.name])], axis=1)\n",
    "combined_df = pd.DataFrame(combined_df.tolist(), columns=result_df.columns, index=result_df.index)\n",
    "print(combined_df)\n",
    "\n",
    "#MLP model accuracy and loss plot\n",
    "#Initialize lists to store loss and accuracy for each fold\n",
    "loss_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "accuracy_per_fold = []\n",
    "val_accuracy_per_fold = []\n",
    "\n",
    "#Extract the loss and accuracy for each fold\n",
    "for history in history_data:\n",
    "    loss_per_fold.append(history['loss'])\n",
    "    val_loss_per_fold.append(history['val_loss'])\n",
    "    accuracy_per_fold.append(history['accuracy'])\n",
    "    val_accuracy_per_fold.append(history['val_accuracy'])\n",
    "\n",
    "#Calculate the values for the accuracy and loss plots\n",
    "def pad_list(data, max_len):\n",
    "    return data + [np.nan] * (max_len - len(data))\n",
    "max_len = max(len(history['loss']) for history in history_data)\n",
    "loss_per_fold = [pad_list(history['loss'], max_len) for history in history_data]\n",
    "val_loss_per_fold = [pad_list(history['val_loss'], max_len) for history in history_data]\n",
    "accuracy_per_fold = [pad_list(history['accuracy'], max_len) for history in history_data]\n",
    "val_accuracy_per_fold = [pad_list(history['val_accuracy'], max_len) for history in history_data]\n",
    "mean_loss = np.nanmean(loss_per_fold, axis=0)\n",
    "std_loss = np.nanstd(loss_per_fold, axis=0)\n",
    "mean_val_loss = np.nanmean(val_loss_per_fold, axis=0)\n",
    "std_val_loss = np.nanstd(val_loss_per_fold, axis=0)\n",
    "mean_accuracy = np.nanmean(accuracy_per_fold, axis=0)\n",
    "std_accuracy = np.nanstd(accuracy_per_fold, axis=0)\n",
    "mean_val_accuracy = np.nanmean(val_accuracy_per_fold, axis=0)\n",
    "std_val_accuracy = np.nanstd(val_accuracy_per_fold, axis=0)\n",
    "\n",
    "#Plot Loss\n",
    "epochs = range(1, len(mean_loss) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, mean_loss, label='Training Loss', color='blue')\n",
    "plt.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, color='blue', alpha=0.2)\n",
    "plt.plot(epochs, mean_val_loss, label='Validation Loss', color='orange')\n",
    "plt.fill_between(epochs, mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, color='orange', alpha=0.2)\n",
    "plt.title('Training and Validation Loss (Mean ± Std)', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "#Plot Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, mean_accuracy, label='Training Accuracy', color='blue')\n",
    "plt.fill_between(epochs, mean_accuracy - std_accuracy, mean_accuracy + std_accuracy, color='blue', alpha=0.2)\n",
    "plt.plot(epochs, mean_val_accuracy, label='Validation Accuracy', color='orange')\n",
    "plt.fill_between(epochs, mean_val_accuracy - std_val_accuracy, mean_val_accuracy + std_val_accuracy, color='orange', alpha=0.2)\n",
    "plt.title('Training and Validation Accuracy (Mean ± Std)', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the models on the selected features from the imbalanced dataset\n",
    "#Initialize lists for storing metrics\n",
    "results = []\n",
    "std = []\n",
    "average_roc_auc = []\n",
    "average_std_roc_auc = []\n",
    "accuracy_scores = []\n",
    "accuracy_std = []\n",
    "f1_scores = []\n",
    "f1_std = []\n",
    "precision_scores = []\n",
    "precision_std = []\n",
    "recall_scores = []\n",
    "recall_std = []\n",
    "MCC_values = []\n",
    "MCC_std = []\n",
    "\n",
    "#Define test set\n",
    "x_test = X_selected_im\n",
    "y_test = y_selected_im\n",
    "\n",
    "#ROC curve plotting settings\n",
    "fpr_mean = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "#Model types and folds\n",
    "model_types = ['DecisionTreeClassifier', 'RandomForestClassifier', 'GaussianNB', 'GradientBoostingClassifier', 'KNeighborsClassifier', 'HistGradientBoostingClassifier', 'MLP']\n",
    "folds_per_model = 5\n",
    "models = []\n",
    "std = [[] for _ in range(len(model_types) * folds_per_model)]\n",
    "conf_matrices = {model_type: [] for model_type in model_types}\n",
    "\n",
    "#Load models from files\n",
    "models = []\n",
    "for model_type in model_types:\n",
    "    for fold in range(folds_per_model):\n",
    "        model_filename = f\"{model_type.replace(' ', '')}_fold_{fold + 1}.joblib\"\n",
    "        try:\n",
    "            loaded_model = joblib.load(model_filename)\n",
    "            models.append(loaded_model)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Model file not found: {model_filename}\")\n",
    "            models.append(None)\n",
    "\n",
    "#Test each model\n",
    "for i, model_type in enumerate(model_types):\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    conf_matrix_sum = np.zeros((2, 2))\n",
    "\n",
    "    for fold in range(folds_per_model):\n",
    "        model = models[i * folds_per_model + fold]\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_prob = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "        #Calculate the different metrics\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        tprs.append(np.interp(fpr_mean, fpr, tpr))\n",
    "        aucs.append(roc_auc)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        MCC_values.append(mcc)\n",
    "        results.append([accuracy, f1, precision, recall, mcc])\n",
    "        current_model_index = i * folds_per_model + fold\n",
    "        std[current_model_index].extend([np.std(y_pred), np.std(f1), np.std(precision), np.std(recall), np.std(mcc)])\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        conf_matrices[model_type].append(conf_matrix)\n",
    "        conf_matrix_sum += conf_matrix\n",
    "\n",
    "    # Calculate values for ROC curve\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(fpr_mean, mean_tpr, label=f'{model_type} (AUC = {mean_auc:.2f} ± {std_auc:.2f})')\n",
    "    plt.fill_between(fpr_mean, mean_tpr - std_tpr, mean_tpr + std_tpr, alpha=0.2)\n",
    "\n",
    "#ROC plot\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for All Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Plot Mean and Standard Deviation Confusion Matrices\n",
    "for model_type, conf_matrices_list in conf_matrices.items():\n",
    "    conf_matrices_array = np.array(conf_matrices_list)\n",
    "    mean_conf_matrix = np.mean(conf_matrices_array, axis=0)\n",
    "    std_conf_matrix = np.std(conf_matrices_array, axis=0)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    sns.heatmap(mean_conf_matrix, annot=True, fmt='.2f', cmap='Blues', xticklabels=['Benign', 'Malicious'], yticklabels=['Benign', 'Malicious'], ax=axes[0])\n",
    "    axes[0].set_title(f'Mean Confusion Matrix for {model_type}')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    sns.heatmap(std_conf_matrix, annot=True, fmt='.2f', cmap='Oranges', xticklabels=['Benign', 'Malicious'], yticklabels=['Benign', 'Malicious'], ax=axes[1])\n",
    "    axes[1].set_title(f'Standard Deviation Confusion Matrix for {model_type}')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Create the table with the results\n",
    "models_names = ['Decision Tree', 'Random Forest', 'Naive Bayes', 'Gradient Boosting', 'KNN', 'Hist Gradient Boosting', 'MLP']\n",
    "DT_results = results[0:5]\n",
    "RF_results = results[5:10]\n",
    "GNB_results = results[10:15]\n",
    "GB_results = results[15:20]\n",
    "KNN_results = results[20:25]\n",
    "HGB_results = results[25:30]\n",
    "MLP_results = results[30:35]\n",
    "DT_std = std[0]\n",
    "RF_std = std[1]\n",
    "GNB_std = std[2]\n",
    "GB_std = std[3]\n",
    "KNN_std = std[4]\n",
    "HGB_std = std[5]\n",
    "MLP_std = std[6]\n",
    "result_data = [\n",
    "    [np.mean(DT_results[0]), np.mean(DT_results[1]), np.mean(DT_results[2]), np.mean(DT_results[3]), np.mean(DT_results[4])],\n",
    "    [np.mean(RF_results[0]), np.mean(RF_results[1]), np.mean(RF_results[2]), np.mean(RF_results[3]), np.mean(RF_results[4])],\n",
    "    [np.mean(GNB_results[0]), np.mean(GNB_results[1]), np.mean(GNB_results[2]), np.mean(GNB_results[3]), np.mean(GNB_results[4])],\n",
    "    [np.mean(GB_results[0]), np.mean(GB_results[1]), np.mean(GB_results[2]), np.mean(GB_results[3]), np.mean(GB_results[4])],\n",
    "    [np.mean(KNN_results[0]), np.mean(KNN_results[1]), np.mean(KNN_results[2]), np.mean(KNN_results[3]), np.mean(KNN_results[4])],\n",
    "    [np.mean(HGB_results[0]), np.mean(HGB_results[1]), np.mean(HGB_results[2]), np.mean(HGB_results[3]), np.mean(HGB_results[4])],\n",
    "    [np.mean(MLP_results[0]), np.mean(MLP_results[1]), np.mean(MLP_results[2]), np.mean(MLP_results[3]), np.mean(MLP_results[4])]\n",
    "]\n",
    "\n",
    "results_std = [\n",
    "    [np.mean(DT_std[0]), np.mean(DT_std[1]), np.mean(DT_std[2]), np.mean(DT_std[3]), np.mean(DT_std[4])],\n",
    "    [np.mean(GB_std[0]), np.mean(GB_std[1]), np.mean(GB_std[2]), np.mean(GB_std[3]), np.mean(GB_std[4])],\n",
    "    [np.mean(KNN_std[0]), np.mean(KNN_std[1]), np.mean(KNN_std[2]), np.mean(KNN_std[3]), np.mean(KNN_std[4])],\n",
    "    [np.mean(GNB_std[0]), np.mean(GNB_std[1]), np.mean(GNB_std[2]), np.mean(GNB_std[3]), np.mean(GNB_std[4])],\n",
    "    [np.mean(RF_std[0]), np.mean(RF_std[1]), np.mean(RF_std[2]), np.mean(RF_std[3]), np.mean(RF_std[4])],\n",
    "    [np.mean(HGB_std[0]), np.mean(HGB_std[1]), np.mean(HGB_std[2]), np.mean(HGB_std[3]), np.mean(HGB_std[4])],\n",
    "    [np.mean(MLP_std[0]), np.mean(MLP_std[1]), np.mean(MLP_std[2]), np.mean(MLP_std[3]), np.mean(MLP_std[4])]\n",
    "]\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "result_df_std = pd.DataFrame(results_std, columns=['accuracy', 'f1', 'precision', 'recall', 'MCC'], index=models_names)\n",
    "combined_df = result_df.apply(lambda row: [f\"{mean:.4f} ± {std:.4f}\" for mean, std in zip(row, result_df_std.loc[row.name])], axis=1)\n",
    "combined_df = pd.DataFrame(combined_df.tolist(), columns=result_df.columns, index=result_df.index)\n",
    "print(combined_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network_traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
